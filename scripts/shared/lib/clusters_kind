# shellcheck shell=bash
# shellcheck disable=SC2034 # As this is dynamically loaded, some variables appear unused

### Variables ###

## Kubernetes version mapping, as supported by kind ##
# See the release notes of the kind version in use
declare -gA kind_k8s_versions
# kind 0.17 hashes
kind_k8s_versions[1.19]=1.19.16@sha256:476cb3269232888437b61deca013832fee41f9f074f9bed79f57e4280f7c48b7
kind_k8s_versions[1.20]=1.20.15@sha256:a32bf55309294120616886b5338f95dd98a2f7231519c7dedcec32ba29699394
# kind 0.18 hashes
kind_k8s_versions[1.21]=1.21.14@sha256:27ef72ea623ee879a25fe6f9982690a3e370c68286f4356bf643467c552a3888
kind_k8s_versions[1.22]=1.22.17@sha256:c8a828709a53c25cbdc0790c8afe12f25538617c7be879083248981945c38693
kind_k8s_versions[1.23]=1.23.17@sha256:e5fd1d9cd7a9a50939f9c005684df5a6d145e8d695e78463637b79464292e66c
kind_k8s_versions[1.24]=1.24.12@sha256:1e12918b8bc3d4253bc08f640a231bb0d3b2c5a9b28aa3f2ca1aee93e1e8db16
kind_k8s_versions[1.25]=1.25.8@sha256:00d3f5314cc35327706776e95b2f8e504198ce59ac545d0200a89e69fce10b7f
kind_k8s_versions[1.26]=1.26.3@sha256:61b92f38dff6ccc29969e7aa154d34e38b89443af1a2c14e6cfbd2df6419c66f
kind_k8s_versions[1.27]=1.27.0@sha256:c6b22e613523b1af67d4bc8a0c38a4c3ea3a2b8fbc5b367ae36345c9cb844518

### Functions ###

function generate_cluster_yaml() {
    # These are used by render_template
    local pod_cidr pod_cidr_ipv6 service_cidr service_cidr_ipv6 dns_domain disable_cni
    pod_cidr="${cluster_CIDRs[${cluster}]}"

    service_cidr="${service_CIDRs[${cluster}]}"

    dns_domain="${cluster}.local"
    disable_cni="false"
    [[ -z "${cluster_cni[$cluster]}" ]] || disable_cni="true"

    local nodes
    for node in ${cluster_nodes[${cluster}]}; do nodes="${nodes}"$'\n'"- role: $node"; done

    if [[ "$DUAL_STACK" ]]; then
        service_cidr_ipv6="${service_IPv6_CIDRs[${cluster}]}"
        pod_cidr_ipv6="${cluster_IPv6_CIDRs[${cluster}]}"
        render_template "${RESOURCES_DIR}/kind-cluster-dual-stack-config.yaml" > "${RESOURCES_DIR}/${cluster}-config.yaml"
    else
        render_template "${RESOURCES_DIR}/kind-cluster-config.yaml" > "${RESOURCES_DIR}/${cluster}-config.yaml"
    fi
}

function kind_fixup_config() {
    local master_ip
    master_ip=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' "${cluster}-control-plane" | head -n 1)
    yq -i ".clusters[0].cluster.server = \"https://${master_ip}:6443\"" "${KUBECONFIG}"
    yq -i "(.. | select(. == \"kind-${cluster}\")) = \"${cluster}\"" "${KUBECONFIG}"
    chmod a+r "${KUBECONFIG}"
}

# In development environments where clusters are brought up and down
# multiple times, several Docker images are repeatedly pulled and deleted,
# leading to the Docker error:
#   "toomanyrequests: You have reached your pull rate limit"
# Preload the KIND image. Also tag it so the `docker system prune` during
# cleanup won't remove it.
function download_kind() {
    if [[ -z "${K8S_VERSION}" ]]; then
        echo "K8S_VERSION not set."
        return
    fi

    # Example: kindest/node:v1.20.7@sha256:cbeaf907fc78ac97ce7b625e4bf0de16e3ea725daf6b04f930bd14c67c671ff9
    kind_image="kindest/node:v${K8S_VERSION}"
    # Example: kindest/node:v1.20.7
    kind_image_tag=kindest/node:v$(echo "${K8S_VERSION}" | awk -F"@" '{print $1}')
    # Example: kindest/node:@sha256:cbeaf907fc78ac97ce7b625e4bf0de16e3ea725daf6b04f930bd14c67c671ff9
    kind_image_sha=kindest/node@$(echo "${K8S_VERSION}" | awk -F"@" '{print $2}')

    # Check if image is already present, and if not, download it.
    echo "Processing Image: $kind_image_tag ($kind_image)"
    if [[ -n $(docker images -q "$kind_image_tag") ]] ; then
        echo "Image $kind_image_tag already downloaded."
        return
    fi

    echo "Image $kind_image_tag not found, downloading..."
    if ! docker pull "$kind_image"; then
        echo "**** 'docker pull $kind_image' failed. Manually run. ****"
        return
    fi

    image_id=$(docker images -q "$kind_image_sha")
    if ! docker tag "$image_id" "$kind_image_tag"; then
        echo "'docker tag ${image_id} ${kind_image_tag}' failed."
    fi
}

function provider_create_cluster() {
    export KUBECONFIG=${KUBECONFIGS_DIR}/kind-config-${cluster}
    rm -f "$KUBECONFIG"

    if kind get clusters | grep -q "^${cluster}$"; then
        echo "KIND cluster already exists, skipping its creation..."
        kind export kubeconfig --name="${cluster}"
        kind_fixup_config
        return
    fi

    echo "Creating KIND cluster..."
    if [[ "${cluster_cni[$cluster]}" == "ovn" ]]; then
        deploy_kind_ovn
        return
    fi

    generate_cluster_yaml
    local image_flag=''
    [[ -z "${K8S_VERSION}" ]] || image_flag="--image=kindest/node:v${K8S_VERSION}"

    kind version
    cat "${RESOURCES_DIR}/${cluster}-config.yaml"
    kind create cluster ${image_flag:+"$image_flag"} --name="${cluster}" --config="${RESOURCES_DIR}/${cluster}-config.yaml"
    kind_fixup_config

    [[ -n "${cluster_cni[$cluster]}" ]] || delete_cluster_on_fail schedule_dummy_pod
    [[ "$LOAD_BALANCER" != true ]] || delete_cluster_on_fail deploy_load_balancer
    [[ "$AIR_GAPPED" != true ]] || air_gap_iptables
}

# This is a workaround and can be removed once we switch the CNI from kindnet to a different one.
# In order to support health-check and hostNetwork use-cases, submariner requires an IPaddress from the podCIDR
# for each node in the cluster. Normally, most of the CNIs create a cniInterface on the host and assign an IP
# from the podCIDR to the interface. Submariner relies on this interface to support the aforementioned use-cases.
# However, with kindnet CNI, it was seen that it does not create a dedicated CNI Interface on the nodes.
# But as soon as a pod is scheduled on a node, it creates a veth-xxx interface which has an IPaddress from the
# podCIDR. In this workaround, we are scheduling a dummy pod as a demonSet on the cluster to trigger the creation
# of this veth-xxx interface which can be used as a cniInterface and we can continue to validate Submariner use-cases.
function schedule_dummy_pod() {
    local ns="subm-kindnet-workaround"
    source "${SCRIPTS_DIR}"/lib/deploy_funcs
    import_image "${REPO}/nettest"

    echo "Creating the ${ns} namespace..."
    kubectl create namespace "${ns}" || :
    deploy_resource "${RESOURCES_DIR}"/dummypod.yaml "$ns"
}

function delete_cluster_on_fail() {
    ( "$@"; ) &
    if ! wait $! ; then
        echo "Failed to run '$*', removing the cluster"
        kubectl cluster-info dump || echo "Can't get cluster info" 1>&2
        kind delete cluster --name="${cluster}"
        return 1
    fi
}

function air_gap_iptables() {
    local DEBUG_PRINT=false

    for node in $(docker ps | grep -w "${cluster}-[-a-z0-9]*" | cut -f1 -d' '); do
        # Allow any non-routable networks, and the globalnet one (240/4)
        for net in 10.0.0.0/8 100.64.0.0/10 127.0.0.0/8 172.16.0.0/12 240.0.0.0/4; do
            docker exec "$node" iptables -A OUTPUT -d "$net" -j ACCEPT
        done

        docker exec "$node" iptables -A OUTPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
        docker exec "$node" iptables -A OUTPUT -j REJECT
    done
}

function deploy_load_balancer() {
    local kind_ip start_ip end_ip net_addr
    kind_ip=$(docker network inspect -f '{{.IPAM.Config}}' kind | awk '/.*/ { print $2 }')
    net_addr=$(echo "${cluster_CIDRs[$cluster]}" | cut -f2 -d'.')
    [[ "$GLOBALNET" != true ]] || net_addr=$(echo "${global_CIDRs[$cluster]}" | cut -f3 -d'.')
    start_ip=$(echo "$kind_ip" | cut -f1-2 -d'.')."$net_addr".100
    end_ip=$(echo "$start_ip" | cut -f1-3 -d'.').250

    kubectl apply -f "https://raw.githubusercontent.com/metallb/metallb/v${METALLB_VERSION}/config/manifests/metallb-native.yaml"
    kubectl wait --for=condition=Ready pods -l app=metallb -n metallb-system --timeout="${TIMEOUT}"
    kubectl apply -f - <<EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: submariner-pool
  namespace: metallb-system
spec:
  addresses:
  - ${start_ip}-${end_ip}
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: submariner-l2
  namespace: metallb-system
EOF
}

function deploy_kind_ovn(){
    export K8s_VERSION="${K8S_VERSION}"
    export NET_CIDR_IPV4="${cluster_CIDRs[${cluster}]}"
    export SVC_CIDR_IPV4="${service_CIDRs[${cluster}]}"
    export KIND_CLUSTER_NAME="${cluster}"

    local ovn_flags=()
    [[ "$OVN_IC" != true ]] || ovn_flags=( -ic -npz 1 -wk 3 )
    delete_cluster_on_fail ./ovn-kubernetes/contrib/kind.sh -ov "$OVN_IMAGE" -cn "${KIND_CLUSTER_NAME}" -ric "${ovn_flags[@]}" -lr -dd "${KIND_CLUSTER_NAME}.local"

    [[ "$AIR_GAPPED" = true ]] && air_gap_iptables
}

function run_local_registry() {
    # Run a local registry to avoid loading images manually to kind
    if registry_running; then
        echo "Local registry $KIND_REGISTRY already running."
        return 0
    fi

    echo "Deploying local registry $KIND_REGISTRY to serve images centrally."
    local volume_dir="/var/lib/registry"
    local volume_flag="/dev/shm/${KIND_REGISTRY}:${volume_dir}"
    selinuxenabled && volume_flag="${volume_flag}:z" 2>/dev/null
    docker run -d -v "${volume_flag}" -p 127.0.0.1:5000:5000 --restart=always --name "$KIND_REGISTRY" registry:2
    docker network connect kind "$KIND_REGISTRY" || true

    # If the local volume mount directory is empty, probably due to a host reboot,
    # then try to push any images with "localhost:5000".
    if [[ -z $(docker exec -e tmp_dir="${volume_dir}" "${KIND_REGISTRY}" /bin/sh -c 'ls -A "${tmp_dir}" 2>/dev/null') ]]; then
        echo "Push images to local registry: $KIND_REGISTRY"
        readarray -t local_image_list < <(docker images | awk -F' ' '/localhost:5000/ {print $1":"$2}')
        for image in "${local_image_list[@]}"; do
            docker push "${image}" || echo "Failed to push ${image@Q} to registry."
        done
    fi
}

function provider_failed() {
    if [[ "$(cat /proc/sys/fs/inotify/max_user_instances)" -lt 512 ]]; then
        echo "Your inotify settings are lower than our recommendation."
        echo "This may cause failures in large deployments, but we don't know if it caused this failure."
        echo "You may need to increase your inotify settings (currently $(cat /proc/sys/fs/inotify/max_user_watches) and $(cat /proc/sys/fs/inotify/max_user_instances)):"
        echo sudo sysctl fs.inotify.max_user_watches=524288
        echo sudo sysctl fs.inotify.max_user_instances=512
        echo 'See https://kind.sigs.k8s.io/docs/user/known-issues/#pod-errors-due-to-too-many-open-files'
    fi
}

function prepare_ovn() {
    export OVN_IMAGE="localhost:5000/ovn-daemonset-f:latest"

    echo "Building ovn-kubernetes with interconnect (OVN-IC) from source"
    echo "This will become unnecessary if OVN CI image publishing is fixed"
    echo "https://github.com/ovn-org/ovn-kubernetes/actions/workflows/docker.yml"
    rm -rf ovn-kubernetes
    git clone https://github.com/ovn-org/ovn-kubernetes
    pushd ovn-kubernetes || exit
    git checkout c6d8579689fe3b15c87abdf7e7647777bad26605

    make -C go-controller

    cp go-controller/_output/go/bin/* dist/images

    echo "ref: $(git rev-parse --symbolic-full-name HEAD) commit: $(git rev-parse HEAD)" > dist/images/git_info
    docker build -t "${OVN_IMAGE}" -f dist/images/Dockerfile.fedora dist/images/
    docker push "${OVN_IMAGE}"

    popd || exit
}

function provider_prepare() {
    [[ -z "${K8S_VERSION}" ]] && K8S_VERSION="${DEFAULT_K8S_VERSION}"
    [[ -n "${kind_k8s_versions[$K8S_VERSION]}" ]] && K8S_VERSION="${kind_k8s_versions[$K8S_VERSION]}"

    download_kind
    run_local_registry
    [[ "${cluster_cni[*]}" != *"ovn"* ]] || prepare_ovn
}
